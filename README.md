# Glasgow ReproducibiliTea chapter
This repo collects materials and announcements for the Glasgow [#ReproducibiliTea](https://reproducibilitea.org/) chapter. 

We do meets for ECRs who are interested in reproducible research.

Sign up with your email address [to be kept updated and be sent the Zoom link](https://forms.office.com/Pages/ResponsePage.aspx?id=KVxybjp2UE-B8i4lTwEzyC_C7bWZ0HxKrMAg9Cgt4G9UMU84VVFCMFdUMkkwT0JQRTU5VDFRTFgwSy4u)

![Glasgow Repro](https://github.com/annahensch/Glasgow-ReproducibiliTea/blob/master/tea_july.jpeg.png)

Link to the July paper: https://tinyurl.com/yaxapfkg

Abstract: This research seeks to understand how survey measurements can represent differences of sex, gender, and sexuality in the UK. It employs a sequential, mixed methods research design featuring three strands. Strand 1 summaries current survey practices by reviewing publicly available information on 22 UK population surveys. Identifying assumptions these surveys make and who they erase. Strand 2 engages with these overlooked groups via focus groups and potentially interviews. It aims to understand the overlooked groups perspectives on how their represented via surveys. Participants will be tasked with creating questions on sex, gender and sexuality that they would be accurately represented by. Strand 3 will test these questions with the broader LGBTI+ population. Featuring an online survey of these communities, recording if/how they respond to the questions constructed by the overlooked groups. It will also test the generalisability of the perspectives and experiences shared by Strand 2's respondents.



![Glasgow Repro](https://github.com/annahensch/Glasgow-ReproducibiliTea/blob/master/TEA_poster.png)

Link to the project website: https://niro-sr.netlify.app/

Abstract: Systematic reviews are complicated. Ensuring that they are reproducible and open is even more complex, and it's easy to overlook things or to adopt a workflow which makes reproducibility and transparency difficult to achieve. Whilst there are many (brilliant!) existing guidelines for conducting systematic reviews, many of these focus on a particular type of research centred around interventions. The NIRO Systematic Review checklist aims to provide a comprehensive set of guidelines for non-interventional research to help bring the gold-standard of evidence synthesis to other areas of research.


![Glasgow Repro](https://github.com/annahensch/Glasgow-ReproducibiliTea/blob/master/scheel.png)

Link to the May paper: https://psyarxiv.com/p6e9c


Abstract: When studies with positive results that support the tested hypotheses have a higher probability of being published than studies with negative results, the literature will give a distorted view of the evidence for scientific claims. Psychological scientists have been concerned about the degree of distortion in their literature due to publication bias and inflated Type-1 error rates. RegisteredReports were developed with the goal to minimise such biases: In this new publication format, peer review and the decision to publish take place before the study results are known.  We compared the results in the full population of published Registered Reports in Psychology(N=71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature (N=152) by searching 633 journals for the phrase ‘test* the hypotheses*’(replicating a method by Fanelli, 2010). Analysing the first hypothesis reported in each paper, we found 96% positive results in standard reports, but only 44% positive results in RegisteredReports. The difference remained nearly as large when direct replications were excluded from the analysis (96% vs 50% positive results). This large gap suggests that psychologists under-report negative results to an extent that threatens cumulative science. Although our study did not directly test the effectiveness of Registered Reports at reducing bias, these results show that the introduction of Registered Reports has led to a much larger proportion of negative results appearing in the published literature compared to standard report.

![Glasgow Repro](https://github.com/annahensch/Glasgow-ReproducibiliTea/blob/master/Andrea%20E.%20Martin%20%26%20Olivia%20Guest(2).png)

Link to the April paper: https://psyarxiv.com/rybh9/  
Link to video recording of the April meeting: https://www.youtube.com/watch?reload=9&v=_WV7EFvFAB8&feature=youtu.be

Olivia and Andrea have written a fantastic blog post answering the questions raised by ECRs during our ReproTea meeting: https://neuroplausible.com/path

Abstract: Psychology endeavors to develop theories of human capacities and behaviors based on a variety of methodologies and dependent measures. We argue that one of the most divisive factors in our field is whether researchers choose to employ computational modeling of theories (over and above data) during the scientific inference process. Modeling is undervalued, yet holds prom- ise for advancing psychological science. The inherent demands of computational modeling guide us towards better science by forcing us to conceptually analyze, specify, and formalise intuitions which otherwise remain unexamined — what we dub “open theory”. Constraining our inference process through modeling enables us to build explanatory and predictive theories. Herein, we present scientific inference in psychology as a path function, where each step shapes the next. Computational modeling can constrain these steps, thus advancing scientific inference over and above stewardship of experimental practice (e.g., preregistration). If psy- chology continues to eschew computational modeling, we predict more replicability “crises” and persistent failure at coherent theory-building. This is because without formal modelling we lack open and transparent theorising. We also explain how to formalise, specify, and implement a computational model, emphasizing that the advantages of modeling can be achieved by anyone with benefit to all.  

